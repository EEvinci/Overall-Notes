# 第二单元：Q-Learning简介 [@EEvinci](https://github.com/EEvinci)

## [术语]：

- **Q-learning：**
  - Q-learning是一种强化学习算法，它通过学习一个动作-价值函数（即Q函数），来确定在每个状态下应该采取哪个动作。在Q-learning中，智能体试图学习一个策略，使得总的奖励最大化。

- **Q-table：**
  - Q-table是一个表，它存储了强化学习智能体对每个状态-动作对的价值估计。表中的每一个条目就代表了在某一状态下采取某一动作的预期回报。

- **状态-价值函数**—— The state-value function ：
  - 对于每个状态，状态-价值函数是如果智能体从当前状态开始，遵循该策略直到结束时的期望回报。
- **动作-价值函数**—— The action-value function ：
  - 与状态-价值函数相比，动作-价值函数不仅考虑了状态，还考虑了在该状态下采取的动作，它计算了智能体在某个状态下执行某个动作后，根据策略所能获得的预期回报。之后智能体会一直遵循这个策略，以最大化回报。
- **Epsilon-greedy 策略 —— Epsilon-greedy strategy ：**
  - 常用的强化学习探索策略，涉及平衡探索和利用。
  - 以 1-epsilon 的概率选择奖励最高的动作。
  - 以 epsilon 的概率选择一个随机动作。
  - Epsilon 通常随着时间减少，以偏向利用。
- **贪婪策略 —— Greedy strategy ：**
  - 涉及总是选择预期会导致最高奖励的动作，基于当前对环境的了解。（只有利用）
  - 总是选择期望奖励最高的动作。
  - 不包括任何探索。
  - 在有不确定性或未知最优动作的环境中可能是不利的。
- **贝尔曼方程 —— The Bellman Equation ：**
  - 贝尔曼方程是描述状态值函数或动作值函数的递归关系。它表明一个状态（或状态-动作对）的值等于即时奖励加上下一状态（或下一状态-动作对）的折扣值的期望。
- **蒙特卡罗 —— Monte Carlo ：**
  - 蒙特卡罗方法是一种通过平均完整经验样本的返回（即经验序列中的总奖励）来估计价值函数的方法。蒙特卡罗方法只在一个完整的序列结束之后才进行更新。
- **时序差分学习 —— Temporal Difference Learning ：**
  - 时序差分学习结合了动态规划和蒙特卡罗方法的思想。在时序差分学习中，智能体不需要等待序列结束就可以更新其价值函数，只需要等待下一个时间步。
- **Frozen-Lake-v1(non-slippery and slippery version) ：**
  - FrozenLake是一种常见的强化学习环境，通常用于测试强化学习算法。在这个环境中，智能体需要在一个冰冻的湖面上移动，目标是从起点移动到目标点。
  - 在non-slippery版本中，智能体的每个动作都会精确地按照预期的方式执行。
  - 在slippery版本中，冰面是滑的，所以智能体的动作可能会导致预期之外的结果。

- **自动驾驶出租车 —— An autonomous taxi：**
  - 在强化学习环境中，自动驾驶出租车是一个经典的问题。在这个问题中，出租车是一个智能体，需要学习如何在城市环境中导航，从一个地方（称为点A）运送乘客到另一个地方（称为点B）。
  - 具体来说，智能体的目标是找到一种策略，能在最少的步骤内完成任务。
  - 自动驾驶出租车的问题模型包括了一个格子世界，其中每个格子代表一个可能的位置。出租车的任务包括寻找乘客，接乘客，然后把乘客送到目的地。出租车需要在这个过程中尽可能减少行驶的步数，因为每走一步都会收到一个负奖励（代表出租车消耗的燃料或者时间成本）。如果成功地将乘客送到目的地，出租车则会得到一个正奖励。

